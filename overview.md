#  Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection

This repository implements the method presented in the paper:  
**"Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection"**  
Huiyi Wang, Fahim Shahriar, Seyed Alireza Azimi, Gautham Vasan, A. Rupam Mahmood, Colin Bellinger  
Accepted at the CoRL 2024 Workshop on Minimalist Robot Learning (MRM-D)  
üìÑ [Read the paper](https://openreview.net/pdf?id=TgXIkK8WPQ)

---

##  Overview

This project investigates how **goal-conditioned reinforcement learning (GCRL)** can be enhanced using **mask-based goal representations** derived from natural language descriptions of target objects. The method enables a single manipulation policy to generalize across a wide variety of objects and goal configurations.

This repository includes:
- A simulation environment for the **UR10e robot**
- Integration with a **physical UR10e robot**
- A trained **mask-conditioned PPO policy**
- Instructions to train in simulation and deploy on hardware

---

##  Key Idea

Traditional GCRL approaches often struggle to generalize to new target objects. This work shows that **binary goal masks**‚Äîeither ground-truth or generated by a pre-trained object grounding model‚Äîenable better generalization and faster learning than alternative goal conditioning strategies such as one-hot vectors or cropped target images.

In particular, we use a pre-trained object grounding model (**GroundingDINO + SAM**) to convert a **textual goal description** (e.g., ‚Äúapple on the right‚Äù) into a **binary mask** that highlights the object‚Äôs location in the scene. This goal mask is updated at every timestep, allowing the agent to:
- Track progress toward the goal
- Receive implicit feedback
- Mitigate the sparse reward problem

The RL policy is conditioned on:
- RGB image
- Proprioceptive state
- Binary goal mask (updated at each timestep)

---

##  Method Summary

- **Text Prompt ‚Üí Grounded Object Detector ‚Üí Binary Mask**  
  Leverages vision-language grounding to generate object-specific goal representations.

- **Goal Conditioning Variants Compared**:
  - One-hot vector (baseline)
  - Goal object image crop
  - **Binary goal mask** (proposed)

- **Learning Algorithm**:  
  PPO (Proximal Policy Optimization) trained on visual, proprioceptive, and mask inputs

---

## Results

| Goal Representation  | Seen Objects (In-Distribution) | Unseen Objects (Out-of-Distribution) |
|----------------------|-------------------------------|--------------------------------------|
| One-hot Vector       | 13%                           | 20%                                  |
| Goal Object Image    | 62%                           | 28%                                  |
| **GT Binary Mask**   | **89%**                       | **90%**                              |

- Binary masks enable strong **zero-shot generalization** to novel target objects.
- Training with **GT masks** transfers well to **DINO-generated masks** on seen objects (~90% success).
- Performance with real-time DINO-generated masks degrades in cluttered scenes due to detection noise.

---

## Key Features

-  **Object grounding** with GroundingDINO + SAM
-  **Mask-based goal representation** for object-level generalization
-  **Single RL policy** conditioned on dynamic, interpretable goal cues
-  Supports **simulation and real-world deployment** with a UR10e robot

---

## Disclaimer

This codebase is provided for research purposes. Users are **fully responsible** for validating and testing any part of the code‚Äîboth in simulation and on real robotic systems.  
The authors and contributors **assume no liability** for any damage, failure, or unexpected behavior that may result from deploying the provided code on physical hardware. Proceed with caution and validate thoroughly in controlled environments.

---

## Code and Usage

Coming soon.

---

## üìù Citation

```bibtex
@inproceedings{
    wang2024goalconditioned,
    title={Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection},
    author={Huiyi Wang and Fahim Shahriar and Seyed Alireza Azimi and Gautham Vasan and A. Rupam Mahmood and Colin Bellinger},
    booktitle={CoRL 2024 Workshop on Minimalist Robot Learning (MRM-D)},
    year={2024},
    url={https://openreview.net/forum?id=TgXIkK8WPQ}
}

#  Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection

This repository implements the method presented in the paper:  
**"Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection"**  
Huiyi Wang, Fahim Shahriar, Seyed Alireza Azimi, Gautham Vasan, A. Rupam Mahmood, Colin Bellinger  
Accepted at the CoRL 2024 Workshop on Minimalist Robot Learning (MRM-D)  
üìÑ [Read the paper](https://openreview.net/pdf?id=TgXIkK8WPQ)

---

##  Overview

This project investigates how **goal-conditioned reinforcement learning (GCRL)** can be enhanced using **mask-based goal representations** derived from natural language descriptions of target objects. The method enables a single manipulation policy to generalize across a wide variety of objects and goal configurations.

This repository includes:
- A simulation environment for the **UR10e robot**
- Integration with a **physical UR10e robot**
- A trained **mask-conditioned PPO policy**
- Instructions to train in simulation and deploy on hardware

---

##  Key Idea

Traditional GCRL approaches often struggle to generalize to new target objects. This work shows that **binary goal masks**‚Äîeither ground-truth or generated by a pre-trained object grounding model‚Äîenable better generalization and faster learning than alternative goal conditioning strategies such as one-hot vectors or cropped target images.

In particular, we use a pre-trained object grounding model (**GroundingDINO + SAM**) to convert a **textual goal description** (e.g., ‚Äúapple on the right‚Äù) into a **binary mask** that highlights the object‚Äôs location in the scene. This goal mask is updated at every timestep, allowing the agent to:
- Track progress toward the goal
- Receive implicit feedback
- Mitigate the sparse reward problem

The RL policy is conditioned on:
- RGB image
- Proprioceptive state
- Binary goal mask (updated at each timestep)

---

##  Method Summary

- **Text Prompt ‚Üí Grounded Object Detector ‚Üí Binary Mask**  
  Leverages vision-language grounding to generate object-specific goal representations.

- **Goal Conditioning Variants Compared**:
  - One-hot vector (baseline)
  - Goal object image crop
  - **Binary goal mask** (proposed)

- **Learning Algorithm**:  
  PPO (Proximal Policy Optimization) trained on visual, proprioceptive, and mask inputs

---

## Results

| Goal Representation  | Seen Objects (In-Distribution) | Unseen Objects (Out-of-Distribution) |
|----------------------|-------------------------------|--------------------------------------|
| One-hot Vector       | 13%                           | 20%                                  |
| Goal Object Image    | 62%                           | 28%                                  |
| **GT Binary Mask**   | **89%**                       | **90%**                              |

- Binary masks enable strong **zero-shot generalization** to novel target objects.
- Training with **GT masks** transfers well to **DINO-generated masks** on seen objects (~90% success).
- Performance with real-time DINO-generated masks degrades in cluttered scenes due to detection noise.

---

## Key Features

-  **Object grounding** with GroundingDINO + SAM
-  **Mask-based goal representation** for object-level generalization
-  **Single RL policy** conditioned on dynamic, interpretable goal cues
-  Supports **simulation and real-world deployment** with a UR10e robot

---

## Disclaimer

This codebase is provided for research purposes. Users are **fully responsible** for validating and testing any part of the code‚Äîboth in simulation and on real robotic systems.  
The authors and contributors **assume no liability** for any damage, failure, or unexpected behavior that may result from deploying the provided code on physical hardware. Proceed with caution and validate thoroughly in controlled environments.

---

## Code and Usage

## Clone the Repository

```bash
git clone https://github.com/cherylwang20/GCRL_UR10e.git
cd GCRL_UR10e
git submodule update --init --recursive
```

### Installation

You would also need to use an external pre-trained object recognition model for object inference. We use GDINO here, the model should be cloned already through submodule. Please allow the instruction link in the GDINO repo to make sure that CUDA with torch and GPU is compatible. 

```bash
cd GroundingDINO
pip install -e .
```

**Note on PyTorch 2.0 Compatibility:**  
If you encounter an error with `value.type()` in `ms_deform_attn_cuda.cu`, replace it with `value.scalar_type()` in:
```
groundingdino/models/GroundingDINO/csrc/MsDeformAttn/ms_deform_attn_cuda.cu
```

## Set Up the Virtual Environment

Use **Python 3.9** (later versions may cause issues with loading the baseline):

```bash
python3.9 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Load Submodules

### UR10e Gym Environment (mj_envs)

```bash
cd mj_envs
pip install -e .
```

## Download the Pre-trained Policy

```bash
mkdir -p policy
gdown 'https://drive.google.com/uc?id=1wKpIUVp2kXvf_Lq1VV7aKIoERLOS6QtW' -O policy/baseline.zip
```

## Use the pretrained policy in Sim


#TODO add script to load policy and run an episode in the ur10e simulation environment with rendering on 
---

## Training a New Policy

To train a new policy, run:
```bash
python training/Train_reach.py --env_name 'UR10eReach1C-v1' --group 'Reach_4C_dt20' --num_envs 4 --learning_rate 0.0003 --clip_range 0.1 --seed=0 --channel_num 4 --fs 20
```
Training Script Arguments

```--env_name 'UR10eReach1C-v1'``` : Specifies the UR10e environment for training.

```--group 'Reach_4C_dt20'``` : Name of the experiment group for logging.

```--num_envs 4``` : Number of parallel environments.

```--learning_rate 0.0003``` : Learning rate for PPO.

```--clip_range 0.1``` : PPO clip range for stable policy updates.

```--seed 0``` : Random seed, often set via SLURM for batch runs.

```--channel_num 4``` : Number of input image channels.

```--fs 20``` : Frame skip (simulation step interval).


## Evaluate an Existing Policy

```bash
python training/Eval_reach_int.py --env_name "UR10eReach1C-v1" --model_num "baseline"
```

## Sim2Real

For achieve effective sim2real transfer, we finetune the policy trained above with observation image augmentation. To train with image augmentation, download the resized external images originally from [OpenX](https://robotics-transformer-x.github.io/) into `background` from https://mcgill-my.sharepoint.com/:u:/g/personal/huiyi_wang_mail_mcgill_ca/EZM8oZL_PPVIiOtrbl8Gy0sBLTBYWjd18TOdrS43WULVdA?e=ZBfhfY. 

#TODO add figure of image mixing

Use the following command:
```bash
python training/Train_reach.py --env_name "UR10eReach1C-v1" --merge True --cont True
```

---

## Details on the Robotic Setup


### Getting Started

- The robot's initial joint configuration is:  
  `[4.7799, -2.0740, 2.6200, 3.0542, -1.5800, 1.4305e-05]` (in radians), with the gripper fully open.
- Place target objects **30‚Äì50 cm in front of the camera**, making sure they are **visible at the start**.
- The camera is mounted on the Robotiq gripper using a custom 3D-printed bracket.  
  It is essential that the **gripper is visible** in the camera view around 17 degrees downwards.

<img src="https://github.com/user-attachments/assets/d3fa1dee-6506-40b1-86d9-40dfb7742a22" alt="Camera Mounting" width="500"/>

- Set the correct IP address for your UR10e robot in:  
  [`GdinoReachGraspEnv_servoJ.py#L86`](https://github.com/cherylwang20/Sim2Real_GCRL_UR10e/blob/3f6d3c6f44f698b062e058aac546f5c7d1629576/src/reachGrasp_env/GdinoReachGraspEnv_servoJ.py#L86)

- Both `servoJ` and `moveJ` motion commands are supported.  
  **`servoJ` offers better performance for sim-to-real transfer.**
- We use a camera resolution of 848 * 480 for best inferenece results and later rescaled to 212 * 120 for policy training.
- Due to exceeding performance, we hardcorded a pick up after approaching close to the table and performing a pick up and drop up: https://github.com/cherylwang20/Sim2Real_GCRL_UR10e/blob/3f6d3c6f44f698b062e058aac546f5c7d1629576/src/reachGrasp_env/GdinoReachGraspEnv_servoJ.py#L326. Uncomment if you don't require this behavior. 



## üìù Citation

```bibtex
@inproceedings{
    wang2024goalconditioned,
    title={Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection},
    author={Huiyi Wang and Fahim Shahriar and Seyed Alireza Azimi and Gautham Vasan and A. Rupam Mahmood and Colin Bellinger},
    booktitle={CoRL 2024 Workshop on Minimalist Robot Learning (MRM-D)},
    year={2024},
    url={https://openreview.net/forum?id=TgXIkK8WPQ}
}
